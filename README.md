# OpenClaw Local LLM Setup

Complete setup guide for running OpenClaw with local LLM (GLM-4.7-Flash) on Debian + Ollama on Windows via Tailscale.

## ğŸ¯ Overview

This repository documents a fully functional OpenClaw setup with:
- **Local LLM**: GLM-4.7-Flash (via Ollama on Windows)
- **Telegram Bot**: Real-time messaging integration
- **GitHub Integration**: Manage repos, issues, PRs via chat
- **Browser Automation**: Playwright Chromium configured
- **Zero API Costs**: Completely local (except optional Claude Haiku)

## ğŸ“Š System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Debian Server (100.108.37.20)                              â”‚
â”‚  â”œâ”€â”€ OpenClaw Gateway (port 18789)                          â”‚
â”‚  â”œâ”€â”€ Telegram Bot (@mightyrajbot)                           â”‚
â”‚  â”œâ”€â”€ Playwright Chromium                                    â”‚
â”‚  â””â”€â”€ ChromaDB (v1.4.1)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                     Tailscale VPN
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Windows PC (100.76.189.18)                                 â”‚
â”‚  â”œâ”€â”€ Ollama Server (port 11434)                             â”‚
â”‚  â”œâ”€â”€ GLM-4.7-Flash:q4_K_M (17.7GB)                          â”‚
â”‚  â””â”€â”€ Nomic-Embed-Text (embeddings)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ… Features

### Working Now:
- âœ… **Chat & Q&A** - Fast responses (20-22 seconds)
- âœ… **Telegram Integration** - @mightyrajbot with streaming
- âœ… **GitHub Management** - Repos, issues, PRs, CI/CD
- âœ… **Web Interface** - http://localhost:18789
- âœ… **Weather Info** - Built-in skill
- âœ… **No Tool Loops** - Stable performance

### Configured (Model-Limited):
- âš ï¸ **Browser Automation** - Configured but local models have limited tool use
- âš ï¸ **Memory** - File-based (ChromaDB ready for future versions)

## ğŸš€ Quick Start

### Prerequisites
- Debian 12 server (or similar Linux)
- Windows PC with Ollama
- Tailscale for networking
- Node.js v24+ (via nvm)

### Installation

```bash
# 1. Install OpenClaw
npm install -g openclaw

# 2. Install Ollama on Windows
# Download from: https://ollama.com

# 3. Pull GLM-4.7-Flash on Windows
ollama pull glm-4.7-flash

# 4. Configure OpenClaw (see docs/)
openclaw setup
```

## ğŸ“š Documentation

Comprehensive guides included:

- **[QUICK_START.md](QUICK_START.md)** - Get started quickly
- **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)** - Common issues & fixes
- **[REDDIT_SOLUTIONS.md](REDDIT_SOLUTIONS.md)** - Community solutions (r/LocalLLaMA)
- **[GITHUB_SOLUTIONS.md](GITHUB_SOLUTIONS.md)** - GitHub Discussion #2936 analysis
- **[ALL_TOOLS_GUIDE.md](ALL_TOOLS_GUIDE.md)** - All 50 OpenClaw skills explained

## ğŸ”§ Key Configuration

### Model Selection
After extensive testing (Reddit + GitHub research):
- âŒ Qwen2.5:14b - Tool calling loops
- âŒ Qwen3:30b - LaTeX formatting issues
- âœ… **GLM-4.7-Flash** - Best performance (2x faster, stable)

### Critical Settings
```json
{
  "models": {
    "providers": {
      "ollama": {
        "baseUrl": "http://100.76.189.18:11434/v1",
        "models": [{
          "id": "glm-4.7-flash:q4_K_M",
          "reasoning": false  // CRITICAL: Must be false!
        }]
      }
    }
  }
}
```

## ğŸ¯ Performance Metrics

| Metric | Value |
|--------|-------|
| Model | GLM-4.7-Flash (9B params, q4_K_M) |
| Response Time | 20-22 seconds |
| Context Window | 32,768 tokens |
| Tool Loops | None (fixed with GLM-4.7) |
| Memory Usage | ~188MB (Debian) + ~5GB (Windows) |
| Cost | $0 (fully local) |

## ğŸ› Known Limitations

1. **Browser Automation**: Local models don't use complex tools well yet
   - **Workaround**: Add Claude Haiku for advanced features (~$0.01/1k tokens)
   
2. **Memory**: File-based only (ChromaDB needs newer OpenClaw)
   - **Status**: ChromaDB installed, ready for future versions

3. **Tool Calling**: Works for simple tools (GitHub, weather)
   - **Complex tools**: May need cloud models

## ğŸ”— Integrations

### Active:
- âœ… Telegram Bot
- âœ… GitHub CLI
- âœ… Playwright Chromium
- âœ… Weather API

### Available (50 skills):
See [ALL_TOOLS_GUIDE.md](ALL_TOOLS_GUIDE.md) for full list

## ğŸ“¦ What's Included

```
openclaw-local-llm-setup/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ QUICK_START.md           # Getting started
â”œâ”€â”€ TROUBLESHOOTING.md       # Issues & fixes
â”œâ”€â”€ REDDIT_SOLUTIONS.md      # Community wisdom
â”œâ”€â”€ GITHUB_SOLUTIONS.md      # GitHub Discussion analysis
â”œâ”€â”€ ALL_TOOLS_GUIDE.md       # All 50 skills explained
â””â”€â”€ .gitignore               # Excludes sensitive data
```

## ğŸ”’ Security

**Excluded from this repo:**
- API keys & tokens
- Bot credentials
- Session data
- Auth profiles
- Personal configurations

**Included safely:**
- Documentation
- Setup guides
- Configuration templates
- Troubleshooting tips

## ğŸ“ Research Summary

This setup is based on:
- Reddit r/LocalLLaMA thread (27 comments analyzed)
- GitHub Discussion #2936 (community solutions)
- Extensive testing of Qwen vs GLM models
- Real-world usage patterns

**Key Finding**: GLM-4.7-Flash outperforms larger Qwen models for tool calling!

## ğŸ¤ Contributing

Found this helpful? Issues and PRs welcome!

## ğŸ“ License

MIT License - Use freely, attribution appreciated

## ğŸ™ Credits

- OpenClaw team
- Ollama project
- Reddit r/LocalLLaMA community
- GitHub Discussion contributors
- GLM-4 by Zhipu AI

## ğŸ“ Support

Check the documentation files for:
- Setup help
- Troubleshooting
- Community solutions
- Advanced configurations

---

**Built by**: Ryuki (@Ryuki0x1)  
**Date**: February 2026  
**Status**: Production-ready âœ…  
**Stars**: â­ If this helped you!
